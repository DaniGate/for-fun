{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the job details of Enron's executives, we want to determine who is likely to be a Person Of Interest (POI).\n",
    "The status of POI (1) or non POI (0) has been manually determined from court investigation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy\n",
    "sys.path.append(\"../../../GitHub/ud120-projects/tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in the dataset: 145\n"
     ]
    }
   ],
   "source": [
    "data_dict = pickle.load(open(\"../../../GitHub/ud120-projects/final_project/final_project_dataset.pkl\", \"r\") )\n",
    "del data_dict[\"TOTAL\"] # Remove TOTAL row outlier\n",
    "print \"Number of people in the dataset:\",len(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the list of features that we have for each Enron executive by selecting one at random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'email_address', 'from_poi_to_this_person']\n"
     ]
    }
   ],
   "source": [
    "features_original = data_dict['METTS MARK'].keys()\n",
    "print features_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Notice that the label to be predicted is the feature 'poi')\n",
    "\n",
    "Create a set of new features derived from those in the dataset, which I believe could provide useful insight on illegal activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for person in data_dict:\n",
    "    poi_from   = data_dict[person]['from_this_person_to_poi']\n",
    "    poi_to   = data_dict[person]['from_poi_to_this_person']\n",
    "    total_from = data_dict[person]['from_messages']\n",
    "    total_to = data_dict[person]['to_messages']\n",
    "\n",
    "    if total_from != 0:\n",
    "        data_dict[person]['fraction_from_poi'] = float(poi_from)/float(total_from)\n",
    "    else:\n",
    "        data_dict[person]['fraction_from_poi'] = float(total_from)\n",
    "\n",
    "    if total_to != 0:\n",
    "        data_dict[person]['fraction_to_poi'] = float(poi_to)/float(total_to)\n",
    "    else:\n",
    "        data_dict[person]['fraction_to_poi'] = float(total_to)\n",
    "\n",
    "    if poi_from != 0:\n",
    "        data_dict[person]['poi_emails_ratio'] = float(poi_to)/float(poi_from)\n",
    "    else:\n",
    "        data_dict[person]['poi_emails_ratio'] = numpy.NaN\n",
    "\n",
    "    if total_from != 0.:\n",
    "        data_dict[person]['total_emails_ratio'] = float(total_to)/float(total_from)\n",
    "    else:\n",
    "        data_dict[person]['total_emails_ratio'] = numpy.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of features I will to investigate into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_list = [ 'poi', 'salary', 'bonus', 'from_this_person_to_poi', 'from_poi_to_this_person', \n",
    "                  'fraction_from_poi', 'fraction_to_poi', 'total_emails_ratio', 'shared_receipt_with_poi',\n",
    "                  'director_fees', 'deferral_payments', 'poi_emails_ratio' ]\n",
    "features_name = features_list[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first divide the dataset beetween features and our target label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dataset = featureFormat(data_dict,features_list)\n",
    "poi, features = targetFeatureSplit( my_dataset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of executives whose activity details are contained in our dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in the dataset: 145\n"
     ]
    }
   ],
   "source": [
    "print \"Number of people in the dataset:\",len(features) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of which, we have the following number of POIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POIs in the dataset: 18.0  ( 12.4137931034 % )\n"
     ]
    }
   ],
   "source": [
    "print \"Number of POIs in the dataset:\",sum(poi),\" (\",sum(poi)/float(len(features))*100.,\"% )\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some of these features against each other to have a feeling of how well-separated are our target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# Regroup every feature in a single list, one value for each person\n",
    "features_split = zip(*features)\n",
    "\n",
    "plot_combinations = [ ('salary','bonus'), ('fraction_from_poi','bonus'), \n",
    "                      ('fraction_from_poi','shared_receipt_with_poi'),\n",
    "                      ('from_poi_to_this_person','fraction_from_poi'),\n",
    "                      ('total_emails_ratio','from_poi_to_this_person'),\n",
    "                      ('salary','shared_receipt_with_poi')]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs  = gridspec.GridSpec(2, 3) \n",
    "\n",
    "for i,(feat1,feat2) in enumerate(plot_combinations):\n",
    "    index1   = features_name.index(feat1)\n",
    "    index2   = features_name.index(feat2)\n",
    "    feature1 = features_split[index1]\n",
    "    feature2 = features_split[index2]\n",
    "    feature1 = numpy.reshape( numpy.array(feature1), (len(feature1), 1))\n",
    "    feature2 = numpy.reshape( numpy.array(feature2), (len(feature2), 1))\n",
    "    plt.subplot(gs[i]).scatter(feature1,feature2,color=\"b\")\n",
    "    \n",
    "    for ii, pp in enumerate(feature1):\n",
    "        if poi[ii]:\n",
    "            plt.scatter(feature1[ii], feature2[ii], color=\"r\")\n",
    "\n",
    "    plt.xlabel(feat1)\n",
    "    plt.ylabel(feat2)\n",
    "\n",
    "    # Show a legend\n",
    "    red_points  = mpatches.Patch(color='red',  label='POI')\n",
    "    blue_points = mpatches.Patch(color='blue', label='non POI')\n",
    "    if i in [0,1,3,4]:\n",
    "        plt.yscale('log')\n",
    "    if i in [1,2,3,4]:\n",
    "        plt.xscale('log')\n",
    "    if i in [0]:\n",
    "        plt.subplot(gs[i]).legend(handles=[blue_points,red_points],loc='lower right')\n",
    "    elif i in [1,3]:\n",
    "        plt.subplot(gs[i]).legend(handles=[blue_points,red_points],loc='lower left')\n",
    "    else:\n",
    "        plt.subplot(gs[i]).legend(handles=[blue_points,red_points],loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features we are interested in are empty for certain executives. This is the number of Enron executives from which we don't know each feature in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN entries for each feature:\n",
      "salary : 0\n",
      "bonus : 0\n",
      "from_this_person_to_poi : 0\n",
      "from_poi_to_this_person : 0\n",
      "fraction_from_poi : 59\n",
      "fraction_to_poi : 59\n",
      "total_emails_ratio : 59\n",
      "shared_receipt_with_poi : 0\n",
      "director_fees : 0\n",
      "deferral_payments : 0\n",
      "poi_emails_ratio : 79\n"
     ]
    }
   ],
   "source": [
    "print \"NaN entries for each feature:\"\n",
    "for (name,feat) in zip(features_name,features_split):\n",
    "    print name,\":\",sum(numpy.isnan(feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this problem, we fill the empty fields with the mean value of the corresponding feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "features_filled = imp.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to test several classification algorithm, but the performance of some of them such as SVC will be non-optimal given that the typical values of each feature can differ by several orders of magnitude. We perform a lineal rescaling to fix this problem. Algotithm that don't need this rescaling won't be affected either positively or negatively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features_rescaled = scaler.fit_transform(features_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now divide our dataset into a 66.7% training sample and a 33.3% testing sample to measure the final performance of our algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "skf = StratifiedKFold(poi,3)\n",
    "i = 0\n",
    "for train, test in skf:\n",
    "    features_train = [ features_rescaled[ii] for ii in train ] \n",
    "    features_test  = [ features_rescaled[ii] for ii in test ]\n",
    "    labels_train   = [ poi[ii] for ii in train ] \n",
    "    labels_test    = [ poi[ii] for ii in test ] \n",
    "    if i == 1: \n",
    "        break\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used StratifiedKFold so there is the same fraction of positive datapoints both in our test and training datasets (~12%). Let's check that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of POIs in the test and train samples: 12.3711340206 % ,  12.5 %\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "print \"Fraction of POIs in the test and train samples:\",\n",
    "print float(sum(labels_train))/float(len(labels_train))*100.,\"% , \",\n",
    "print float(sum(labels_test))/float(len(labels_test))*100.,\"%\"\n",
    "print sum(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now select the 5 most important features for our dataset, which are those with the highest score on a SelectKBest algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction_from_poi score: 7.98 ( p = 5.8e-03 )\n",
      "salary score: 7.06 ( p = 9.2e-03 )\n",
      "bonus score: 3.78 ( p = 5.5e-02 )\n",
      "from_this_person_to_poi score: 2.03 ( p = 1.6e-01 )\n",
      "shared_receipt_with_poi score: 1.76 ( p = 1.9e-01 )\n",
      "director_fees score: 1.31 ( p = 2.6e-01 )\n",
      "deferral_payments score: 1.13 ( p = 2.9e-01 )\n",
      "poi_emails_ratio score: 0.56 ( p = 4.6e-01 )\n",
      "from_poi_to_this_person score: 0.56 ( p = 4.6e-01 )\n",
      "fraction_to_poi score: 0.51 ( p = 4.8e-01 )\n",
      "total_emails_ratio score: 0.14 ( p = 7.1e-01 )\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "selector = SelectKBest(k=6)\n",
    "X_train = selector.fit_transform(features_train, labels_train)\n",
    "X_test  = selector.transform(features_test)\n",
    "\n",
    "feat_score = selector.scores_.tolist()\n",
    "feat_pval = selector.pvalues_.tolist()\n",
    "\n",
    "for fs,name,fp in sorted(zip(feat_score,features_name,feat_pval),reverse=True):\n",
    "    print \"%s score: %.2f ( p = %.1e )\" % (name,fs,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected indexes: salary , bonus , from_this_person_to_poi , fraction_from_poi , shared_receipt_with_poi , director_fees ,\n"
     ]
    }
   ],
   "source": [
    "feat_support = selector.get_support().tolist()\n",
    "feat_selected = []\n",
    "print \"Selected indexes:\",\n",
    "for f in [ fl for (fl,fs) in zip(features_name,feat_support) if fs ]:\n",
    "    print f,\",\",\n",
    "    feat_selected.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use these 6 features to predict who is and who is not a POI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our fraction of positive labels (POIs) is pretty low, our evaluation of the algorithm performance can not be based on its accuracy. Let's select a proper evaluation metric that fits the intended result interpretation:\n",
    "\n",
    "I would like to help the investigation by pinning down possible suspects for a closer police investigation, so my algorithm finds all the possible POIs even if I have to pay the price of a few false positives, i.e. accusing innocent people. Algorithms are not providing the final veredict about these executives' fraud activities, and so those innocent Enron employees will be discarded later on after a deeper fiscal investigation.\n",
    "\n",
    "Therefore, we will optimize our algorithm for a maximum recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "score = 'recall'\n",
    "best_performance = { }\n",
    "best_algorithm = { }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test the performance of the following classification algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_names = [ 'GaussianNB' , 'DecisionTreeClassifier', 'LogisticRegression', 'SVC' ]\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifiers      = [ GaussianNB() , DecisionTreeClassifier(), LogisticRegression(), SVC() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test each of these algorithms, as well as several combination of tunning parameters, using a cross-validated grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_grid = [ {  },\n",
    "               { 'criterion':         [ 'gini' , 'entropy' ],\n",
    "                 'max_depth':         [ 15, 8 , 6 , 4 ] ,\n",
    "                 'min_samples_split': [ 10 , 6 , 4 , 2] } ,\n",
    "               { 'C': [ 1000 , 100 , 10 , 5 , 1 ] } ,\n",
    "               { 'kernel': [ 'linear', 'rbf', 'sigmoid'] ,\n",
    "                 'C':      [ 1000, 100, 10, 5, 1] ,\n",
    "                 'gamma': [ 0.5 , 1.0 , 10. ] }\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> GaussianNB :: Tuning parameters for recall\n",
      " Best parameters set found on training set: {}\n",
      " recall scores:\n",
      "0.794 (+/-0.800) for {}\n",
      "\n",
      ">>> DecisionTreeClassifier :: Tuning parameters for recall\n",
      " Best parameters set found on training set: {'min_samples_split': 2, 'criterion': 'gini', 'max_depth': 8}\n",
      " recall scores:\n",
      "0.263 (+/-0.806) for {'min_samples_split': 10, 'criterion': 'gini', 'max_depth': 15}\n",
      "0.206 (+/-0.800) for {'min_samples_split': 6, 'criterion': 'gini', 'max_depth': 15}\n",
      "0.206 (+/-0.800) for {'min_samples_split': 4, 'criterion': 'gini', 'max_depth': 15}\n",
      "0.263 (+/-0.806) for {'min_samples_split': 2, 'criterion': 'gini', 'max_depth': 15}\n",
      "0.206 (+/-0.800) for {'min_samples_split': 10, 'criterion': 'gini', 'max_depth': 8}\n",
      "0.263 (+/-0.806) for {'min_samples_split': 6, 'criterion': 'gini', 'max_depth': 8}\n",
      "0.206 (+/-0.800) for {'min_samples_split': 4, 'criterion': 'gini', 'max_depth': 8}\n",
      "0.299 (+/-0.917) for {'min_samples_split': 2, 'criterion': 'gini', 'max_depth': 8}\n",
      "0.263 (+/-0.806) for {'min_samples_split': 10, 'criterion': 'gini', 'max_depth': 6}\n",
      "0.263 (+/-0.806) for {'min_samples_split': 6, 'criterion': 'gini', 'max_depth': 6}\n",
      "0.206 (+/-0.800) for {'min_samples_split': 4, 'criterion': 'gini', 'max_depth': 6}\n",
      "0.263 (+/-0.806) for {'min_samples_split': 2, 'criterion': 'gini', 'max_depth': 6}\n",
      "0.263 (+/-0.806) for {'min_samples_split': 10, 'criterion': 'gini', 'max_depth': 4}\n",
      "0.149 (+/-0.640) for {'min_samples_split': 6, 'criterion': 'gini', 'max_depth': 4}\n",
      "0.206 (+/-0.800) for {'min_samples_split': 4, 'criterion': 'gini', 'max_depth': 4}\n",
      "0.263 (+/-0.806) for {'min_samples_split': 2, 'criterion': 'gini', 'max_depth': 4}\n",
      "0.093 (+/-0.600) for {'min_samples_split': 10, 'criterion': 'entropy', 'max_depth': 15}\n",
      "0.093 (+/-0.600) for {'min_samples_split': 6, 'criterion': 'entropy', 'max_depth': 15}\n",
      "0.149 (+/-0.640) for {'min_samples_split': 4, 'criterion': 'entropy', 'max_depth': 15}\n",
      "0.149 (+/-0.640) for {'min_samples_split': 2, 'criterion': 'entropy', 'max_depth': 15}\n",
      "0.093 (+/-0.600) for {'min_samples_split': 10, 'criterion': 'entropy', 'max_depth': 8}\n",
      "0.253 (+/-0.806) for {'min_samples_split': 6, 'criterion': 'entropy', 'max_depth': 8}\n",
      "0.149 (+/-0.640) for {'min_samples_split': 4, 'criterion': 'entropy', 'max_depth': 8}\n",
      "0.093 (+/-0.600) for {'min_samples_split': 2, 'criterion': 'entropy', 'max_depth': 8}\n",
      "0.093 (+/-0.600) for {'min_samples_split': 10, 'criterion': 'entropy', 'max_depth': 6}\n",
      "0.253 (+/-0.806) for {'min_samples_split': 6, 'criterion': 'entropy', 'max_depth': 6}\n",
      "0.149 (+/-0.640) for {'min_samples_split': 4, 'criterion': 'entropy', 'max_depth': 6}\n",
      "0.149 (+/-0.640) for {'min_samples_split': 2, 'criterion': 'entropy', 'max_depth': 6}\n",
      "0.093 (+/-0.600) for {'min_samples_split': 10, 'criterion': 'entropy', 'max_depth': 4}\n",
      "0.196 (+/-0.800) for {'min_samples_split': 6, 'criterion': 'entropy', 'max_depth': 4}\n",
      "0.186 (+/-0.800) for {'min_samples_split': 4, 'criterion': 'entropy', 'max_depth': 4}\n",
      "0.093 (+/-0.600) for {'min_samples_split': 2, 'criterion': 'entropy', 'max_depth': 4}\n",
      "\n",
      ">>> LogisticRegression :: Tuning parameters for recall\n",
      " Best parameters set found on training set: {'C': 1000}\n",
      " recall scores:\n",
      "0.000 (+/-0.000) for {'C': 1000}\n",
      "0.000 (+/-0.000) for {'C': 100}\n",
      "0.000 (+/-0.000) for {'C': 10}\n",
      "0.000 (+/-0.000) for {'C': 5}\n",
      "0.000 (+/-0.000) for {'C': 1}\n",
      "\n",
      ">>> SVC :: Tuning parameters for recall\n",
      " Best parameters set found on training set: {'kernel': 'sigmoid', 'C': 1000, 'gamma': 10.0}\n",
      " recall scores:\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 1000, 'gamma': 0.5}\n",
      "0.093 (+/-0.600) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.5}\n",
      "0.160 (+/-0.640) for {'kernel': 'sigmoid', 'C': 1000, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 1000, 'gamma': 1.0}\n",
      "0.242 (+/-0.806) for {'kernel': 'rbf', 'C': 1000, 'gamma': 1.0}\n",
      "0.103 (+/-0.600) for {'kernel': 'sigmoid', 'C': 1000, 'gamma': 1.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 1000, 'gamma': 10.0}\n",
      "0.242 (+/-0.806) for {'kernel': 'rbf', 'C': 1000, 'gamma': 10.0}\n",
      "0.320 (+/-0.800) for {'kernel': 'sigmoid', 'C': 1000, 'gamma': 10.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 100, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.5}\n",
      "0.103 (+/-0.600) for {'kernel': 'sigmoid', 'C': 100, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 100, 'gamma': 1.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'rbf', 'C': 100, 'gamma': 1.0}\n",
      "0.103 (+/-0.600) for {'kernel': 'sigmoid', 'C': 100, 'gamma': 1.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 100, 'gamma': 10.0}\n",
      "0.242 (+/-0.806) for {'kernel': 'rbf', 'C': 100, 'gamma': 10.0}\n",
      "0.263 (+/-0.806) for {'kernel': 'sigmoid', 'C': 100, 'gamma': 10.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 10, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'sigmoid', 'C': 10, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 10, 'gamma': 1.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'rbf', 'C': 10, 'gamma': 1.0}\n",
      "0.103 (+/-0.600) for {'kernel': 'sigmoid', 'C': 10, 'gamma': 1.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 10, 'gamma': 10.0}\n",
      "0.242 (+/-0.806) for {'kernel': 'rbf', 'C': 10, 'gamma': 10.0}\n",
      "0.160 (+/-0.640) for {'kernel': 'sigmoid', 'C': 10, 'gamma': 10.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 5, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'rbf', 'C': 5, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'sigmoid', 'C': 5, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 5, 'gamma': 1.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'rbf', 'C': 5, 'gamma': 1.0}\n",
      "0.103 (+/-0.600) for {'kernel': 'sigmoid', 'C': 5, 'gamma': 1.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 5, 'gamma': 10.0}\n",
      "0.186 (+/-0.800) for {'kernel': 'rbf', 'C': 5, 'gamma': 10.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'sigmoid', 'C': 5, 'gamma': 10.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 1, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'sigmoid', 'C': 1, 'gamma': 0.5}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 1, 'gamma': 1.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'rbf', 'C': 1, 'gamma': 1.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'sigmoid', 'C': 1, 'gamma': 1.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'linear', 'C': 1, 'gamma': 10.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'rbf', 'C': 1, 'gamma': 10.0}\n",
      "0.000 (+/-0.000) for {'kernel': 'sigmoid', 'C': 1, 'gamma': 10.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_score = 0.\n",
    "\n",
    "for index,classifier in enumerate(classifiers):\n",
    "    print \">>> %s :: Tuning parameters for %s\" % (classifier_names[index],score)\n",
    "    clf = GridSearchCV(classifier,param_grid[index],scoring=\"%s\"%(score),cv=10)\n",
    "\n",
    "    clf.fit(X_train,labels_train)\n",
    "\n",
    "    print \" Best parameters set found on training set:\",\n",
    "    print clf.best_params_\n",
    "    print \"\",score,\"scores:\"\n",
    "    for params, mean_score, cv_scores in clf.grid_scores_:\n",
    "        print \"%0.3f (+/-%0.03f) for %r\" % (mean_score, cv_scores.std() * 2, params)\n",
    "    print \"\"\n",
    "    best_performance[classifier_names[index]] = \"%0.3f with parameters %r\" % (clf.best_score_, clf.best_params_)\n",
    "    \n",
    "    if clf.best_score_ > max_score:\n",
    "        max_score = clf.best_score_\n",
    "        best_algorithm[max_score] = (classifier,clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the maximum recall obtained by each of the algorithm, together with the optimum tunning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For optimum recall in our training set:\n",
      "   GaussianNB :   0.794 with parameters {}\n",
      "   DecisionTreeClassifier :   0.299 with parameters {'min_samples_split': 2, 'criterion': 'gini', 'max_depth': 8}\n",
      "   LogisticRegression :   0.000 with parameters {'C': 1000}\n",
      "   SVC :   0.320 with parameters {'kernel': 'sigmoid', 'C': 1000, 'gamma': 10.0}\n"
     ]
    }
   ],
   "source": [
    "print \"For optimum\",score,\"in our training set:\"\n",
    "for name in classifier_names:\n",
    "    print \"  \",name,\":  \",best_performance[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have obtained similar performance with the 4 classifiers but a maximum recall of 79.4% has been obtained using a simple Gaussian Naive Bayes classifier. Let's now measure the performance of this algorithm using the held-out test subsample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting POIs on the test set using a GaussianNB classifier\n",
      "Parameters: {}\n",
      "Predictions obtained in 0.001s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.14      0.25        42\n",
      "        1.0       0.14      1.00      0.25         6\n",
      "\n",
      "avg / total       0.89      0.25      0.25        48\n",
      "\n",
      "\t\t      Predicted\n",
      "\t\t non-POI\tPOI\n",
      "True\tnon-POI:  6 \t\t36 \t\t\n",
      "\tPOI:\t   0 \t\t6 \t\t\n",
      "Recall: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "selected_classifier = best_algorithm[max_score][0]\n",
    "selected_parameters = best_algorithm[max_score][1]\n",
    "\n",
    "print \"Predicting POIs on the test set using a\",classifier_names[classifiers.index(selected_classifier)],\"classifier\"\n",
    "print \"Parameters:\",selected_parameters\n",
    "t0 = time()\n",
    "clf = selected_classifier.set_params(**selected_parameters)\n",
    "clf.fit(X_train,labels_train)\n",
    "labels_pred = clf.predict(X_test)\n",
    "print \"Predictions obtained in %0.3fs\" % (time() - t0)\n",
    "\n",
    "print classification_report(labels_test, labels_pred)\n",
    "\n",
    "print \"\\t\\t      Predicted\"\n",
    "print \"\\t\\t non-POI\\tPOI\"\n",
    "for ii,row in enumerate(confusion_matrix(labels_test, labels_pred, labels=(0,1))):\n",
    "    if ii == 0:\n",
    "        print \"True\\tnon-POI: \",\n",
    "    if ii == 1:\n",
    "        print \"\\tPOI:\\t  \",\n",
    "    for element in row:\n",
    "        print element,\"\\t\\t\",\n",
    "    print \"\"\n",
    "    \n",
    "print \"Recall:\",recall_score(labels_test,labels_pred)*100.,\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
